---
title: "Making ASVs and a Phyloseq Object"
date: "`r format(Sys.time(), '%d %B, %Y')`"
author: "Augustus Pendleton"
editor_options: 
  chunk_output_type: console
---

# Loading packages 
```{r load-packages}
# Efficient loading of the packages 
pacman::p_load(patchwork, dada2, phyloseq, Biostrings, tidyverse, install = FALSE)

```

# Set the path to the seq files 

```{r set-path}
# Set path to the gzipped files 
path = "sequencing_files"
```

# Load in Forward and Reverse reads and assess the quality

```{r read-and-quality, fig.width = 8, fig.height = 8}
# 1. Forward read variable 
forward_reads <- 
  list.files(path,
           pattern = "_1.fastq.gz",
           full.names = TRUE)

reverse_reads <- 
  list.files(path,
           pattern = "_2.fastq.gz",
           full.names = TRUE)

sample_names <- 
  basename(forward_reads) %>%
  str_remove("_1.fastq.gz") %>%
  str_remove("_2.fastq.gz")

# Create a variable holding file names for the Forward and Reverse filtered reads 

filtered_filepaths_forward <- 
  file.path("filtered", paste0(sample_names, "_R1_filtered.fastq.gz"))

filtered_filepaths_reverse <- 
  file.path("filtered", paste0(sample_names, "_R2_filtered.fastq.gz"))

# Show the quality of each base on the reads
set.seed(314)
random_plots <- sample(1:77, size = 6)


forward_qual_plots <-
      plotQualityProfile(forward_reads[random_plots])

reverse_qual_plots <-
      plotQualityProfile(reverse_reads[random_plots])


write_rds(forward_qual_plots,
          file = "forward_qual_plots.rds",
          compress = "gz")

write_rds(reverse_qual_plots,
          file = "reverse_qual_plots.rds",
          compress = "gz")

forward_qual_plots <- read_rds("forward_qual_plots.rds")
reverse_qual_plots <- read_rds("reverse_qual_plots.rds")

forward_qual_plots
reverse_qual_plots

```

# Filter and Trim

These sequences don't have primers (yay!). Amplicon length is 464, so for overlap we need at least 232 on each side. We'll do 275 for forward and 250 for reverse. 
```{r filter-trim}

set.seed(031491)

filtered_outs <-
  filterAndTrim(forward_reads,
              filtered_filepaths_forward,
              reverse_reads,
              filtered_filepaths_reverse,
                            truncLen = c(280,280),
                            maxN = 0,
                            maxEE = Inf,
                            truncQ = 2,
                            rm.phix = TRUE,
                            compress = TRUE,
                            multithread = 30
                            )


write_rds(filtered_outs,
          file = "filtered_outs.rds",
          compress = "gz")

filtered_outs <- read_rds("filtered_outs.rds")


filt_forward_qual_plots <-
      plotQualityProfile(filtered_filepaths_forward[random_plots])

filt_reverse_qual_plots <-
      plotQualityProfile(filtered_filepaths_reverse[random_plots])


write_rds(filt_forward_qual_plots,
          file = "filt_forward_qual_plots.rds",
          compress = "gz")

write_rds(filt_reverse_qual_plots,
          file = "filt_reverse_qual_plots.rds",
          compress = "gz")

filt_forward_qual_plots <- read_rds("filt_forward_qual_plots.rds")
filt_reverse_qual_plots <- read_rds("filt_reverse_qual_plots.rds")

filt_forward_qual_plots
filt_reverse_qual_plots

filtered_outs %>%
  as.data.frame() %>%
  summarize(Median_Reads_In = median(reads.in),
            Median_Reads_Out = median(reads.out),
            Median_Reads_Removed = Median_Reads_In - Median_Reads_Out,
            Percent_Removed = Median_Reads_Removed / Median_Reads_In)


```

When we did this before with more (to my mind) reasonable settings (truncLen = c(275, 250), maxEE = c(2,2)), we removed over 90% of reads, and ended up with just less thatn 150 ASVs by the time we got to merging. With these very lenient models, we only remove 35% of reads...but will likely infer some erroneous ASVs. I'm not sure the best route forward here. Tbh, the amplicon length is too large (>420bp) for how poor the sequencing quality is - there just isn't a clean solution. Though Props 2016 only demonstrate richnesses in the 20s...so perhaps this is reasonable? (Again, though, OTUs vs ASVs).

Did we lose any samples?

```{r check-samples-complete-removal}
filtered_outs %>%
  as.data.frame() %>%
  dplyr::filter(reads.out == 0) %>%
  rownames() %>%
  str_remove("_1.fastq.gz")

```

No.

# Generate an error model 
```{r learn-errors}

error_models_forward <-
        learnErrors(filtered_filepaths_forward,
                    multithread = 30,
                    verbose = FALSE
                    )
error_models_reverse <-
        learnErrors(filtered_filepaths_reverse,
                    multithread = 30,
                    verbose = FALSE
                    )
write_rds(error_models_forward, file = "error_models_forward.rds")
write_rds(error_models_reverse, file = "error_models_reverse.rds")

error_models_forward <- read_rds("error_models_forward.rds")
error_models_reverse <- read_rds("error_models_reverse.rds")
# Plot the errors
# the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected
plotErrors(error_models_forward,
           nominalQ = TRUE)

plotErrors(error_models_reverse,
           nominalQ = TRUE)
```

Predicted error hews fairly close to observed error, which is good.

# Inferring ASVs on the forward and reverse sequences 

```{r infer-ASVs}


dada_forwards <-
  dada(filtered_filepaths_forward,
       err = error_models_forward,
       multithread = 60)


dada_reverses <-
  dada(filtered_filepaths_reverse,
       err = error_models_reverse,
       multithread = 60)


write_rds(dada_forwards,
          file = "dada_forwards.rds",
          compress = "gz")

write_rds(dada_reverses,
          file = "dada_reverses.rds",
          compress = "gz")

dada_forwards <- read_rds("dada_forwards.rds")
dada_reverses <- read_rds("dada_reverses.rds")

```


# Merge forward and reverse ASVs 
```{r merge-FandR-ASVs}

merged_amplicons <-
    mergePairs(dada_forwards,
               filtered_filepaths_forward,
               dada_reverses,
               filtered_filepaths_reverse,
               verbose = FALSE)
write_rds(merged_amplicons,
          file = "merged_amplicons.rds",
          compress = "gz")

merged_amplicons <- read_rds("merged_amplicons.rds")
```


# Generate a count table! 

```{r gen-countTable-seqTab}

seqtabs <- makeSequenceTable(merged_amplicons)

seqtabs %>%
        getSequences %>%
        nchar %>%
        table %>%
        sort
```

Okay, even with very light read-removal, we still don't get a ton of ASVs, which actually makes me feel a lot better. We'll move forward with this. Props removed everything smaller than 400bp, which we'll do as well. 

# Check & Remove for Chimeras (Bimeras)

```{r check-chimeras}

seqtab_nochim <- removeBimeraDenovo(seqtabs,
                                    multithread = 60)

write_rds(seqtab_nochim, file = "seqtab_nochim.rds")

seqtab_nochim <- read_rds("seqtab_nochim.rds")


asvs <- dim(seqtab_nochim)[2]

# What proportion of counts were removed? 
chim_check <- sum(seqtab_nochim)/sum(seqtabs)
frac_removed <- (1-chim_check)*100

```

We removed `r frac_removed`% of reads when filtering for chimaeras. 


# Size Selection

I also want to get rid of ASVs that are too big.

```{r size selection}
asv_keeps <- nchar(getSequences(seqtab_nochim)) >= 400 


seqtab_nc_len <- seqtab_nochim[,asv_keeps]


```

We removed all ASVs that were smaller than 400bp.

# Track the sequences through the pipeline 


```{r track-seqs-loess}
# create a little function to identify number seqs 
getN <- function(x) sum(getUniques(x))


dada_forward_reads <- 
  map_dbl(dada_forwards, getN) 

dada_reverse_reads <- 
    map_dbl(dada_reverses, getN) 

merged_reads <- 
    map_dbl(merged_amplicons, getN) 

sum_filter_df <- 
  filtered_outs %>%
  as.data.frame() %>%
  rownames_to_column(var = "sample_name") %>%
  mutate(sample_name = str_remove(sample_name, "_1.fastq.gz")) %>%
  group_by(sample_name) %>%
  summarize(input = sum(reads.in),
            filtered = sum(reads.out)) %>%
  dplyr::filter(filtered > 0)

# Make the table to track the seqs 
track_df <- sum_filter_df %>%
  cbind(dada_forward_reads,
        dada_reverse_reads,
        merged_reads,
        rowSums(seqtab_nc_len))


head(track_df)

# Change column names 
colnames(track_df) <- c("sample" , "input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")
rownames(track_df) <- track_df$sample

# Generate a plot to track the reads through our DADA2 pipeline
track_df %>%
  pivot_longer(input:nochim, names_to = "read_type", values_to = "num_reads") %>%
  #left_join(metadata, by = "sample_name") %>% 
  mutate(read_type = fct_relevel(read_type, 
                                 "input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")) %>%
  ggplot(aes(x = read_type, y = num_reads, fill = read_type)) + 
  #facet_grid(~strata) + 
  geom_line(aes(group = sample)) + 
  geom_point(shape = 21, size = 3, alpha = 0.8, color = "grey") + 
  scale_fill_brewer(palette = "Spectral") + 
  theme_bw() + 
  labs(x = "Filtering Step", y = "Number of Sequences") +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```

Some samples absolutely drop off the face of the earth after getting filtered, while others are fairly good? Why?

Let's bring in some metadata to see what's going on.
```{r find-metadaat}

props_meta_raw <- read_csv("props_meta.csv")

clean_meta <- props_meta_raw %>%
  select(sample = Run, Reactor_cycle, sample_num = `Sample Name`, Reactor_phase, temp, timepoint, Description)

props_abund_raw <- read_csv("props_abundance.csv")


clean_abund <- props_abund_raw %>%
  select(sample_num = `sample_title*`, cell_per_ml = `Cell density (cells/mL)`, d0 = `Hill diversity: D0`, d1 = `Hill diversity: D1`) %>%
  mutate(sample_num = as.character(sample_num))
```


```{r}

# make track dataframe and edit names to match metadata
track_df_final <- track_df %>%
  mutate(perc_reads_retained = 100 * nochim / input) %>%
  left_join(clean_meta) %>%
  left_join(clean_abund)


track_df_final %>%
  ggplot(aes(x = Reactor_phase, y = perc_reads_retained)) + 
  ggbeeswarm::geom_beeswarm()

track_df_final %>%
  ggplot(aes(x = Reactor_cycle, y = perc_reads_retained)) + 
  ggbeeswarm::geom_beeswarm()

track_df_final %>%
  ggplot(aes(x = Description, y = perc_reads_retained)) + 
  ggbeeswarm::geom_beeswarm()


track_df_final %>%
  ggplot(aes(x = d0, y = perc_reads_retained)) + 
  geom_point()

low_reads <- track_df_final %>%
  slice_min(n = 4, order_by = perc_reads_retained) %>%
  pull(sample)

high_reads <- track_df_final %>%
  slice_max(n = 4, order_by = perc_reads_retained) %>%
  pull(sample)


plotQualityProfile(forward_reads[str_detect(forward_reads, paste0(c(low_reads, high_reads), collapse = "|"))])

plotQualityProfile(reverse_reads[str_detect(reverse_reads, paste0(c(low_reads, high_reads), collapse = "|"))])

plotQualityProfile(filtered_filepaths_forward[str_detect(forward_reads, paste0(c(low_reads, high_reads), collapse = "|"))])

plotQualityProfile(filtered_filepaths_reverse[str_detect(reverse_reads, paste0(c(low_reads, high_reads), collapse = "|"))])

```

Like, the quality doesn't look that significantly different to me, comparing the samples which kept a lot of their reads vs. the samples which lost a lot of their reads. And like we see after filtering that all of the reads have been removed, but good lord I do not know why. 

If this was my own data, there would be a lot more detective work to do. Since this is just one part of a larger metaanalysis, I think I'm just going to remove those samples and move forward with the samples that we do have. 


```{r}

hist(track_df_final$perc_reads_retained, breaks = 50)

new_track_df <- track_df_final %>%
  dplyr::filter(perc_reads_retained > 40)


new_seqtab1 <- seqtab_nc_len[str_detect(row.names(seqtab_nc_len), paste0(new_track_df$sample, collapse = "|")),] 

new_seqtab2 <- new_seqtab1[,colSums(new_seqtab1) > 0]

dim(new_seqtab2)
```

Okay...we only keep 47 out of 77 samples, and maintain 216 ASVs.

# Taxonomic Assignment

```{r assign-taxa}

taxa <- assignTaxonomy(new_seqtab2, "/workdir/databases/silva_v138/version_2/silva_nr99_v138.2_toGenus_trainset.fa.gz",
                       multithread=60,
                       tryRC = TRUE)

new_tax_tab <- taxa %>%
  as.data.frame() %>%
  rownames_to_column(var = "ASVseqs") 
head(new_tax_tab)

# intution check 
stopifnot(new_tax_tab$ASVseqs == colnames(new_seqtab2))

asv_headers <- vector(dim(new_seqtab2)[2], mode = "character")

# loop through vector and fill it in with ASV names 

for (i in 1:dim(new_seqtab2)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep = "_")
}

# intitution check
head(asv_headers, 20)


##### Rename ASVs in table then write out our ASV fasta file! 

asv_tab <- t(new_seqtab2)


## Rename our asvs! 
row.names(asv_tab) <- sub(">", "", asv_headers)

# Now let's add the ASV names 
rownames(new_tax_tab) <- rownames(asv_tab)


### Final prep of tax table. Add new column with ASV names 
asv_tax <- 
  new_tax_tab %>%
  # add rownames from count table for phyloseq handoff
  mutate(ASV = rownames(asv_tab)) %>%
  # Resort the columns with select
  dplyr::select(Kingdom, Phylum, Class, Order, Family, Genus, ASV, ASVseqs)

# Intution check
stopifnot(asv_tax$ASV == rownames(asv_tax), rownames(asv_tax) == rownames(asv_tab))

```

Make a phyloseq object

```{r}

colnames(asv_tab) <- str_remove(colnames(asv_tab), "_R1_filtered.fastq.gz")

as.matrix(asv_tax)
row.names(new_track_df) <- new_track_df$sample

props_phylo <- phyloseq(
  otu_table(asv_tab, taxa_are_rows = TRUE),
  tax_table(as.matrix(asv_tax)),
  sample_data(new_track_df)
)

clean_props_phylo <- 
  props_phylo %>%
  subset_taxa(Family != "Mitochondria" | is.na(Family)) %>%
  subset_taxa(Order !="Chloroplast" | is.na(Order)) 

# Removed one ASV that was a Chloroplast
save(clean_props_phylo, file =  "clean_props_phylo.RData")
```
